# π0.5

This page describes how to use model-opt to export, compute, and infer the `pi05 model`.

## More information about openpi usage: [openpi link](https://github.com/Physical-Intelligence/openpi)

### 1. prepare

#### 1.1  replace transforms with `openpi transformers`

```shell
cp -r ${YOUR_OPENPI_CODEPATH}/src/openpi/models_pytorch/transformers_replace/* /opt/openpi/lib/python3.12/site-packages/transformers/
```

#### 1.2 Get pi05 `pytorch` model from jax model.
[How to export pi05 model to pytorch detail page link](https://github.com/Physical-Intelligence/openpi?tab=readme-ov-file#pytorch-support)


### 2. Run pi05 pytorch model

```shell
python scripts/deployment/pi05/standalone_inference_script.py --model_path /srcs/sources/opensrc/robot/openpi/pytorch_pi05_libero/ --inference-mode pytorch --perf
```

+ output:
```shell
Inference time: 0.5023 seconds
e2e 494.89 ± 6.61 ms (shared)
```

### 3. Model export to onnx

##### We can use model-opt to export the PI05 model as three onnx models: vit, llm, and action expert.

#### 3.1 vit onnx model export

```shell
model-opt export --model_name pi05_libero/vit --model_path /srcs/sources/opensrc/robot/openpi/pytorch_pi05_libero/ --export_dir /tmp/export/pi05
```

+ `/srcs/sources/opensrc/robot/openpi/pytorch_pi05_libero/` is your pi05 pytorch model path get from `1.2`.

+ `--model_name`: the model you want to export, like `pi05_libero/vit`, `pi05_libero/llm`, `pi05_libero/expert`

##### output:

```shell
pi05 model name: pi05_libero
Start Pi05Model load...
...
Pi05Model load done.
Start Vit export onnx...
Vit export onnx done to /tmp/export/pi05 cost:4.453001499176025s
```

#### 3.2 llm onnx model export

```shell
model-opt export --model_name pi05_libero/llm --model_path /srcs/openpi/pytorch_pi05_libero/ --export_dir /tmp/export/pi05
```

##### output:
```shell
Start Pi05Model load...
...
load pytorch model....
compile time:0.004251083009876311
Pi05Model load done.
Start LLM export onnx...
LLM export onnx done to /tmp/export/pi05 cost:11.542186975479126s
```

#### 3.3 action expert onnx model export

+ Due to a bug in the `openpi modeling_gemma.py` implementation, GemmaModel cannot use repr, therefore the following modifications are made:

`error like below:`
```shell
File "/opt/openpi/lib/python3.12/site-packages/transformers/models/gemma/modeling_gemma.py", line 107, in extra_repr
```

`modify codes:`
```python
    def extra_repr(self):
        if self.dense is not None:
            repr_str = f"eps={self.eps}, adaptive=True, cond_dim={self.cond_dim}"
        else:
            repr_str = f"{tuple(self.weight.shape)}, eps={self.eps}"
        return repr_str
```

```shell
model-opt export --model_name pi05_libero/expert --model_path /srcs/openpi/pytorch_pi05_libero/ --export_dir /tmp/export/pi05/
```

##### success output:
```shell
[torch.onnx] Run decomposition... ✅
[torch.onnx] Translate the graph into ONNX...
[torch.onnx] Translate the graph into ONNX... ✅
Expert export onnx done to /tmp/export/pi05/ dtype:torch.bfloat16 cost:45.03810739517212
```

### 4. build onnx to tensorrt engine

#### 4.1 llm onnx build

```shell
model-opt build  --model_path /tmp/export/pi05/llm.onnx --build_cfg config/build_configs/llm_build_cfg.py --export_dir /tmp/build/pi05/llm.engine
```

+ `--model_path`: the exported onnx model path
+ `--export_dir`: build tensorrt engine fullname
+ `--build_cfg`: build cfg, you can refer to [llm_build_cfg.py](../config/build_configs/llm_build_cfg.py)

+ success output:

```shell
Saving engine to /tmp/build/pi05/llm.engine...
2026-02-12 12:29:29,787 - INFO - Engine saved! Size: 3784.35 MB
Engine saved! Size: 3784.35 MB
2026-02-12 12:29:29,787 - INFO - 
================================================================================

================================================================================
2026-02-12 12:29:29,787 - INFO - ENGINE BUILD COMPLETE!
ENGINE BUILD COMPLETE!
2026-02-12 12:29:29,787 - INFO - ================================================================================
================================================================================
2026-02-12 12:29:29,787 - INFO - Engine file: /tmp/build/pi05/llm.engine
Engine file: /tmp/build/pi05/llm.engine
2026-02-12 12:29:29,787 - INFO - Size: 3784.35 MB
Size: 3784.35 MB
2026-02-12 12:29:29,787 - INFO - Build time: 24.0s
Build time: 24.0s
2026-02-12 12:29:29,787 - INFO - Precision: BF16
Precision: BF16
2026-02-12 12:29:29,787 - INFO - ================================================================================
```


##### 4.1.1 inference with llm.engine
```shell
python scripts/deployment/pi05/standalone_inference_script.py --model-path /srcs/openpi/pytorch_pi05_libero/ --inference-mode tensorrt --trt_engine_path /tmp/build/pi05/ --llm-engine llm.engine --perf
```

+ `--inference-mode`: use tensorrt engine
+ `--trt_engine_path`: .engine file path
+ `--llm-engine`: llm tensort engine filename

#### 4.2 vit and action expert onnx build

```shell
# vit build
model-opt build  --model_path /tmp/export/pi05/vit.onnx --build_cfg config/vit_build_cfg.py --export_dir /tmp/build/pi05/vit.engine

# expert build
model-opt build  --model_path /tmp/export/pi05/expert.onnx --build_cfg config/build_configs/expert_build_cfg.py --export_dir /tmp/build/pi05/expert.engine
```

#### 4.3 run with vit,llm,expert tensorrt engine
```shell
python scripts/deployment/pi05/standalone_inference_script.py --model-path /srcs/openpi/pytorch_pi05_libero/ --inference-mode tensorrt --trt_engine_path /tmp/build/pi05/ --llm-engine llm_968.engine --expert_engine expert_968.engine  --vit_engine vit.engine --perf
```

+ output:

```shell
total time: 8.03 ± 0.14 ms
Inference time: 0.2012 seconds
e2e 199.85 ± 1.31 ms (shared)
suffix 1.25 ± 0.06 ms (shared)
action 10.83 ± 0.29 ms (shared)
embed_prefix 25.71 ± 0.57 ms (shared)
lang_emb 0.27 ± 0.04 ms (shared)
llm 56.15 ± 0.10 ms (shared
```

### 5. Eval model

#### 5.1 convert dataset format.

```shell
python -m lerobot.datasets.v21.convert_dataset_v20_to_v21 --repo-id=physical-intelligence/libero
```

#### 5.2 compute norm stats
```shell
python /srcs/sources/opensrc/robot/openpi/scripts/compute_norm_stats.py --config-name=pi05_libero
```

#### 5.3 run eval

```shell
model-opt eval --model_name pi05_libero --model_path /srcs/sources/opensrc/robot/openpi/pytorch_pi05_libero/ --dataset pi05_libero --output_dir /tmp/eval/pi05/
```

[pi05 quantize](./pi05_quantize.md)

## Q and A

#### Q: download `gs://big_vision/paligemma_tokenizer.model` failed. 

```shell
  File "/opt/openpi/lib/python3.12/site-packages/gcsfs/retry.py", line 113, in validate_response
    raise HttpError(error)
gcsfs.retry.HttpError: Anonymous caller does not have storage.objects.get access to the Google Cloud Storage object. Permission 'storage.objects.get' denied on resource (or it may not exist)., 401

```

#### A: use http_proxy or copy it from another place

```shell
cp -rf /srcs/.cache/openpi/big_vision/* /root/.cache/openpi/big_vision/
```


#### Q: use `torch.compile` failed in Jetson Thor

```shell
torch._inductor.exc.InductorError: NoTritonConfigsError: No valid triton configs. PTXASError: PTXAS error: Internal Triton PTX codegen error
`ptxas` stderr:
ptxas-blackwell fatal   : Value 'sm_110a' is not defined for option 'gpu-name'
```

#### A: use a wrong ptxa compiler, set `TRITON_PTXAS_PATH`

```shell
export TRITON_PTXAS_PATH=/usr/local/cuda/bin/ptxas
```